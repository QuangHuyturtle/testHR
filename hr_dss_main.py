#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HR Decision Support System - Main Application (Vietnamese Version)
Há»‡ thá»‘ng há»— trá»£ ra quyáº¿t Ä‘á»‹nh tuyá»ƒn dá»¥ng nhÃ¢n sá»± - PhiÃªn báº£n tiáº¿ng Viá»‡t

Author: Student
Date: 2025
Course: Há»‡ há»— trá»£ ra quyáº¿t Ä‘á»‹nh, Há»‡ Ä‘iá»u hÃ nh vÃ  láº­p trÃ¬nh Linux
"""

import pandas as pd
import numpy as np
import pickle
import json
import os
import sys
import logging
from datetime import datetime
import argparse
from pathlib import Path

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline

# Text processing
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK data if not exists
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("Äang táº£i dá»¯ liá»‡u NLTK...")
    nltk.download('punkt')
    nltk.download('stopwords')
    print("âœ“ Táº£i dá»¯ liá»‡u NLTK hoÃ n táº¥t")

# Setup logging
os.makedirs('logs', exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/hr_dss.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class HRDecisionSupportSystem:
    """
    Há»‡ thá»‘ng há»— trá»£ ra quyáº¿t Ä‘á»‹nh tuyá»ƒn dá»¥ng nhÃ¢n sá»±
    """
    
    def __init__(self, model_path="models/", data_path="data/"):
        """
        Khá»Ÿi táº¡o há»‡ thá»‘ng HR DSS
        
        Args:
            model_path (str): ÄÆ°á»ng dáº«n lÆ°u mÃ´ hÃ¬nh
            data_path (str): ÄÆ°á»ng dáº«n dá»¯ liá»‡u
        """
        self.model_path = Path(model_path)
        self.data_path = Path(data_path)
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
        self.model_path.mkdir(exist_ok=True)
        self.data_path.mkdir(exist_ok=True)
        
        # Initialize components
        self.model = None
        self.vectorizer = None
        self.scaler = None
        self.label_encoder = None
        self.feature_columns = None
        
        # Load model if exists
        self.load_model()
        
        logger.info("ğŸš€ Há»‡ thá»‘ng Há»— trá»£ Ra quyáº¿t Ä‘á»‹nh Tuyá»ƒn dá»¥ng Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o")
    
    def preprocess_text(self, text):
        """
        Tiá»n xá»­ lÃ½ vÄƒn báº£n (CV, mÃ´ táº£ ká»¹ nÄƒng)
        
        Args:
            text (str): VÄƒn báº£n cáº§n xá»­ lÃ½
            
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xá»­ lÃ½
        """
        if pd.isna(text) or text is None:
            return ""
        
        # Chuyá»ƒn vá» chá»¯ thÆ°á»ng
        text = str(text).lower()
        
        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i vÃ  sá»‘
        text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Tokenize vÃ  loáº¡i bá» stop words
        try:
            tokens = word_tokenize(text)
            stop_words = set(stopwords.words('english'))
            tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
            return ' '.join(tokens)
        except:
            return text
    
    def extract_features(self, df):
        """
        TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u á»©ng viÃªn
        
        Args:
            df (pd.DataFrame): Dá»¯ liá»‡u á»©ng viÃªn
            
        Returns:
            pd.DataFrame: Dá»¯ liá»‡u Ä‘Ã£ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng
        """
        logger.info("ğŸ“Š Äang trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u á»©ng viÃªn...")
        
        # Táº¡o báº£n sao Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng dá»¯ liá»‡u gá»‘c
        processed_df = df.copy()
        
        # Tiá»n xá»­ lÃ½ vÄƒn báº£n
        processed_df['skills_processed'] = processed_df['skills'].apply(self.preprocess_text)
        processed_df['experience_description_processed'] = processed_df['experience_description'].apply(self.preprocess_text)
        
        # Káº¿t há»£p cÃ¡c trÆ°á»ng vÄƒn báº£n
        processed_df['combined_text'] = (
            processed_df['skills_processed'] + ' ' + 
            processed_df['experience_description_processed']
        )
        
        # TÃ­nh toÃ¡n cÃ¡c Ä‘áº·c trÆ°ng sá»‘
        processed_df['education_score'] = processed_df['education_level'].map({
            'high_school': 1,
            'associate': 2, 
            'bachelor': 3,
            'master': 4,
            'phd': 5
        }).fillna(1)
        
        # Chuáº©n hÃ³a kinh nghiá»‡m (nÄƒm)
        processed_df['years_experience'] = pd.to_numeric(processed_df['years_experience'], errors='coerce').fillna(0)
        
        # TÃ­nh Ä‘iá»ƒm ká»¹ nÄƒng (sá»‘ lÆ°á»£ng ká»¹ nÄƒng)
        processed_df['num_skills'] = processed_df['skills'].str.count(',') + 1
        processed_df['num_skills'] = processed_df['num_skills'].fillna(0)
        
        logger.info("âœ“ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng hoÃ n táº¥t")
        return processed_df
    
    def create_sample_data(self, num_samples=1000):
        """
        Táº¡o dá»¯ liá»‡u máº«u cho training
        
        Args:
            num_samples (int): Sá»‘ lÆ°á»£ng máº«u
            
        Returns:
            pd.DataFrame: Dá»¯ liá»‡u máº«u
        """
        logger.info(f"ğŸ² Táº¡o dá»¯ liá»‡u máº«u vá»›i {num_samples} máº«u...")
        
        np.random.seed(42)
        
        # Danh sÃ¡ch ká»¹ nÄƒng phá»• biáº¿n
        skills_pool = [
            'python', 'java', 'javascript', 'sql', 'machine learning', 'data analysis',
            'project management', 'communication', 'teamwork', 'leadership',
            'react', 'nodejs', 'docker', 'kubernetes', 'aws', 'azure',
            'agile', 'scrum', 'git', 'linux', 'statistics', 'excel'
        ]
        
        education_levels = ['high_school', 'associate', 'bachelor', 'master', 'phd']
        positions = ['developer', 'analyst', 'manager', 'designer', 'consultant']
        
        data = []
        
        for i in range(num_samples):
            # Random features
            years_exp = np.random.randint(0, 20)
            education = np.random.choice(education_levels)
            position = np.random.choice(positions)
            
            # Random skills (2-8 skills per candidate)
            num_skills = np.random.randint(2, 9)
            candidate_skills = np.random.choice(skills_pool, num_skills, replace=False)
            skills_str = ', '.join(candidate_skills)
            
            # Experience description
            exp_desc = f"Worked as {position} for {years_exp} years with expertise in {', '.join(candidate_skills[:3])}"
            
            # Decision logic for labeling
            score = 0
            if years_exp >= 3: score += 2
            if education in ['bachelor', 'master', 'phd']: score += 2
            if num_skills >= 5: score += 1
            if 'python' in candidate_skills or 'java' in candidate_skills: score += 1
            if 'leadership' in candidate_skills or 'project management' in candidate_skills: score += 1
            
            # Label: suitable if score >= 4
            suitable = 1 if score >= 4 else 0
            
            data.append({
                'candidate_id': f'CAND_{i+1:04d}',
                'years_experience': years_exp,
                'education_level': education,
                'skills': skills_str,
                'experience_description': exp_desc,
                'position_applied': position,
                'suitable': suitable
            })
        
        df = pd.DataFrame(data)
        
        # Save sample data
        sample_file = self.data_path / 'sample_candidates.csv'
        df.to_csv(sample_file, index=False)
        logger.info(f"âœ“ Dá»¯ liá»‡u máº«u Ä‘Ã£ lÆ°u táº¡i {sample_file}")
        
        return df
    
    def train_model(self, df=None):
        """
        Training mÃ´ hÃ¬nh phÃ¢n loáº¡i vá»›i thÃ´ng bÃ¡o tiáº¿ng Viá»‡t
        """
        logger.info("ğŸ§  Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh...")
        
        if df is None:
            logger.info("ğŸ“¦ Táº¡o dá»¯ liá»‡u máº«u...")
            df = self.create_sample_data()
        
        # Extract features
        logger.info("ğŸ”§ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u...")
        processed_df = self.extract_features(df)
        
        # Prepare text features
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        
        logger.info("ğŸ“ Xá»­ lÃ½ Ä‘áº·c trÆ°ng vÄƒn báº£n...")
        text_features = self.vectorizer.fit_transform(processed_df['combined_text'])
        
        # Prepare numerical features
        numerical_cols = ['education_score', 'years_experience', 'num_skills']
        self.feature_columns = numerical_cols
        
        if self.scaler is None:
            self.scaler = StandardScaler()
        
        logger.info("ğŸ“ Chuáº©n hÃ³a Ä‘áº·c trÆ°ng sá»‘...")
        numerical_features = self.scaler.fit_transform(processed_df[numerical_cols])
        
        # Combine features
        X_text = text_features.toarray()
        X_numerical = numerical_features
        X = np.hstack([X_text, X_numerical])
        
        # Target variable
        y = processed_df['suitable']
        
        logger.info(f"ğŸ“Š Tá»•ng sá»‘ máº«u: {len(X)}")
        logger.info(f"ğŸ¯ Sá»‘ Ä‘áº·c trÆ°ng: {X.shape[1]}")
        logger.info(f"âš–ï¸ PhÃ¢n phá»‘i nhÃ£n - PhÃ¹ há»£p: {y.sum()}, ChÆ°a phÃ¹ há»£p: {len(y) - y.sum()}")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        logger.info(f"ğŸ”¨ Dá»¯ liá»‡u huáº¥n luyá»‡n: {len(X_train)} máº«u")
        logger.info(f"ğŸ” Dá»¯ liá»‡u kiá»ƒm thá»­: {len(X_test)} máº«u")
        
        # Train model
        logger.info("ğŸŒ² Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest...")
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced'
        )
        
        self.model.fit(X_train, y_train)
        
        # Evaluate model
        logger.info("ğŸ“ˆ ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh...")
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        logger.info(f"ğŸ‰ Huáº¥n luyá»‡n mÃ´ hÃ¬nh hoÃ n táº¥t!")
        logger.info(f"ğŸ† Äá»™ chÃ­nh xÃ¡c: {accuracy:.3f}")
        logger.info(f"ğŸ“‹ BÃ¡o cÃ¡o phÃ¢n loáº¡i:\n{classification_report(y_test, y_pred)}")
        
        # Save model
        logger.info("ğŸ’¾ LÆ°u mÃ´ hÃ¬nh...")
        self.save_model()
        
        return accuracy
    
    def predict_candidate(self, candidate_data):
        """
        Dá»± Ä‘oÃ¡n Ä‘á»™ phÃ¹ há»£p cá»§a á»©ng viÃªn vá»›i thÃ´ng bÃ¡o tiáº¿ng Viá»‡t
        
        Args:
            candidate_data (dict): ThÃ´ng tin á»©ng viÃªn
            
        Returns:
            dict: Káº¿t quáº£ dá»± Ä‘oÃ¡n báº±ng tiáº¿ng Viá»‡t
        """
        if self.model is None:
            raise ValueError("âŒ MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y cháº¡y train_model() trÆ°á»›c.")
        
        # Convert to DataFrame
        df = pd.DataFrame([candidate_data])
        
        # Extract features
        processed_df = self.extract_features(df)
        
        # Prepare features
        text_features = self.vectorizer.transform(processed_df['combined_text'])
        numerical_features = self.scaler.transform(processed_df[self.feature_columns])
        
        # Combine features
        X_text = text_features.toarray()
        X_numerical = numerical_features
        X = np.hstack([X_text, X_numerical])
        
        # Predict
        prediction = self.model.predict(X)[0]
        probability = self.model.predict_proba(X)[0]
        
        result = {
            'candidate_id': candidate_data.get('candidate_id', 'KhÃ´ng xÃ¡c Ä‘á»‹nh'),
            'prediction': 'Suitable' if prediction == 1 else 'Not Suitable',
            'prediction_vietnamese': 'PhÃ¹ há»£p' if prediction == 1 else 'ChÆ°a phÃ¹ há»£p',
            'confidence': max(probability),
            'probability_suitable': probability[1] if len(probability) > 1 else probability[0],
            'recommendation': self.get_recommendation(prediction, max(probability)),
            'recommendation_vietnamese': self.get_recommendation_vietnamese(prediction, max(probability)),
            'education_display': self.get_education_vietnamese(candidate_data.get('education_level', '')),
            'summary': self.generate_candidate_summary_vietnamese(candidate_data, prediction, max(probability))
        }
        
        logger.info(f"ğŸ¯ Dá»± Ä‘oÃ¡n cho {result['candidate_id']}: {result['prediction_vietnamese']} (Ä‘á»™ tin cáº­y: {result['confidence']:.3f})")
        
        return result
    
    def get_recommendation(self, prediction, confidence):
        """
        ÄÆ°a ra khuyáº¿n nghá»‹ dá»±a trÃªn dá»± Ä‘oÃ¡n (English)
        """
        if prediction == 1:
            if confidence > 0.8:
                return "Highly recommended for interview"
            elif confidence > 0.6:
                return "Recommended for interview"
            else:
                return "Consider for interview with caution"
        else:
            if confidence > 0.8:
                return "Not recommended"
            else:
                return "Need further evaluation"
    
    def get_recommendation_vietnamese(self, prediction, confidence):
        """
        ÄÆ°a ra khuyáº¿n nghá»‹ báº±ng tiáº¿ng Viá»‡t dá»±a trÃªn dá»± Ä‘oÃ¡n
        
        Args:
            prediction (int): Káº¿t quáº£ dá»± Ä‘oÃ¡n (0 hoáº·c 1)
            confidence (float): Äá»™ tin cáº­y
            
        Returns:
            str: Khuyáº¿n nghá»‹ báº±ng tiáº¿ng Viá»‡t
        """
        if prediction == 1:
            if confidence > 0.8:
                return "Ráº¥t khuyáº¿n khÃ­ch má»i phá»ng váº¥n"
            elif confidence > 0.6:
                return "Khuyáº¿n khÃ­ch má»i phá»ng váº¥n"
            else:
                return "CÃ¢n nháº¯c má»i phá»ng váº¥n vá»›i thÃ¡i Ä‘á»™ tháº­n trá»ng"
        else:
            if confidence > 0.8:
                return "KhÃ´ng khuyáº¿n khÃ­ch"
            else:
                return "Cáº§n Ä‘Ã¡nh giÃ¡ thÃªm"

    def get_education_vietnamese(self, education_level):
        """Chuyá»ƒn Ä‘á»•i trÃ¬nh Ä‘á»™ há»c váº¥n sang tiáº¿ng Viá»‡t"""
        education_mapping = {
            'high_school': 'Tá»‘t nghiá»‡p THPT',
            'associate': 'Cao Ä‘áº³ng', 
            'bachelor': 'Cá»­ nhÃ¢n',
            'master': 'Tháº¡c sÄ©',
            'phd': 'Tiáº¿n sÄ©'
        }
        return education_mapping.get(education_level, 'KhÃ´ng xÃ¡c Ä‘á»‹nh')

    def generate_candidate_summary_vietnamese(self, candidate_data, prediction, confidence):
        """Táº¡o tÃ³m táº¯t á»©ng viÃªn báº±ng tiáº¿ng Viá»‡t"""
        years_exp = candidate_data.get('years_experience', 0)
        education = self.get_education_vietnamese(candidate_data.get('education_level', ''))
        skills_count = len(candidate_data.get('skills', '').split(','))
        
        summary = f"á»¨ng viÃªn cÃ³ {years_exp} nÄƒm kinh nghiá»‡m, trÃ¬nh Ä‘á»™ {education}, "
        summary += f"sá»Ÿ há»¯u {skills_count} ká»¹ nÄƒng chÃ­nh. "
        
        if prediction == 1:
            if confidence > 0.8:
                summary += "ÄÃ¢y lÃ  á»©ng viÃªn tiá»m nÄƒng cao, ráº¥t phÃ¹ há»£p vá»›i vá»‹ trÃ­ á»©ng tuyá»ƒn."
            else:
                summary += "á»¨ng viÃªn cÃ³ tiá»m nÄƒng tá»‘t, phÃ¹ há»£p vá»›i vá»‹ trÃ­ á»©ng tuyá»ƒn."
        else:
            if confidence > 0.8:
                summary += "á»¨ng viÃªn chÆ°a Ä‘Ã¡p á»©ng Ä‘á»§ yÃªu cáº§u cho vá»‹ trÃ­ nÃ y."
            else:
                summary += "á»¨ng viÃªn cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ ká»¹ hÆ¡n Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng."
        
        return summary
    
    def batch_predict(self, csv_file):
        """
        Dá»± Ä‘oÃ¡n hÃ ng loáº¡t tá»« file CSV
        
        Args:
            csv_file (str): ÄÆ°á»ng dáº«n file CSV
            
        Returns:
            pd.DataFrame: Káº¿t quáº£ dá»± Ä‘oÃ¡n
        """
        logger.info(f"ğŸ“ Xá»­ lÃ½ dá»± Ä‘oÃ¡n hÃ ng loáº¡t tá»« file {csv_file}")
        
        df = pd.read_csv(csv_file)
        results = []
        
        total_candidates = len(df)
        logger.info(f"ğŸ‘¥ Tá»•ng sá»‘ á»©ng viÃªn cáº§n xá»­ lÃ½: {total_candidates}")
        
        for idx, row in df.iterrows():
            candidate_data = row.to_dict()
            try:
                result = self.predict_candidate(candidate_data)
                results.append(result)
                
                if (idx + 1) % 10 == 0:
                    logger.info(f"â³ ÄÃ£ xá»­ lÃ½ {idx + 1}/{total_candidates} á»©ng viÃªn")
                    
            except Exception as e:
                logger.error(f"âŒ Lá»—i dá»± Ä‘oÃ¡n á»©ng viÃªn {candidate_data.get('candidate_id', idx)}: {e}")
                results.append({
                    'candidate_id': candidate_data.get('candidate_id', f'CAND_{idx}'),
                    'prediction': 'Error',
                    'prediction_vietnamese': 'Lá»—i',
                    'confidence': 0.0,
                    'probability_suitable': 0.0,
                    'recommendation': 'Processing error',
                    'recommendation_vietnamese': 'Lá»—i xá»­ lÃ½'
                })
        
        results_df = pd.DataFrame(results)
        
        # Save results
        output_file = self.data_path / f'ket_qua_du_doan_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ Káº¿t quáº£ dá»± Ä‘oÃ¡n hÃ ng loáº¡t Ä‘Ã£ lÆ°u táº¡i {output_file}")
        
        return results_df
    
    def save_model(self):
        """
        LÆ°u mÃ´ hÃ¬nh vÃ  cÃ¡c thÃ nh pháº§n
        """
        try:
            # Save model components
            with open(self.model_path / 'rf_model.pkl', 'wb') as f:
                pickle.dump(self.model, f)
            
            with open(self.model_path / 'vectorizer.pkl', 'wb') as f:
                pickle.dump(self.vectorizer, f)
            
            with open(self.model_path / 'scaler.pkl', 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # Save feature columns
            with open(self.model_path / 'feature_columns.json', 'w') as f:
                json.dump(self.feature_columns, f)
            
            logger.info("âœ… MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u thÃ nh cÃ´ng")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i lÆ°u mÃ´ hÃ¬nh: {e}")
    
    def load_model(self):
        """
        Load mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u vá»›i thÃ´ng bÃ¡o tiáº¿ng Viá»‡t
        """
        try:
            if (self.model_path / 'rf_model.pkl').exists():
                logger.info("ğŸ“‚ Äang táº£i mÃ´ hÃ¬nh tá»« file...")
                
                with open(self.model_path / 'rf_model.pkl', 'rb') as f:
                    self.model = pickle.load(f)
                
                with open(self.model_path / 'vectorizer.pkl', 'rb') as f:
                    self.vectorizer = pickle.load(f)
                
                with open(self.model_path / 'scaler.pkl', 'rb') as f:
                    self.scaler = pickle.load(f)
                
                with open(self.model_path / 'feature_columns.json', 'r') as f:
                    self.feature_columns = json.load(f)
                
                logger.info("âœ… Táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng!")
                return True
                
        except Exception as e:
            logger.warning(f"âš ï¸ KhÃ´ng thá»ƒ táº£i mÃ´ hÃ¬nh: {e}")
            return False
    
    def generate_report(self, results_df=None):
        """
        Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p báº±ng tiáº¿ng Viá»‡t
        """
        if results_df is None:
            logger.warning("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u káº¿t quáº£ Ä‘á»ƒ táº¡o bÃ¡o cÃ¡o")
            return {}
        
        total_candidates = len(results_df)
        suitable_candidates = len(results_df[results_df['prediction'] == 'Suitable'])
        avg_confidence = results_df['confidence'].mean()
        
        report = {
            'total_candidates': total_candidates,
            'suitable_candidates': suitable_candidates,
            'unsuitable_candidates': total_candidates - suitable_candidates,
            'suitable_percentage': (suitable_candidates / total_candidates * 100) if total_candidates > 0 else 0,
            'average_confidence': avg_confidence,
            'high_confidence_suitable': len(results_df[
                (results_df['prediction'] == 'Suitable') & 
                (results_df['confidence'] > 0.8)
            ]),
            'timestamp': datetime.now().strftime("%d/%m/%Y %H:%M:%S"),
            'summary': f"ÄÃ£ xá»­ lÃ½ {total_candidates} á»©ng viÃªn, trong Ä‘Ã³ {suitable_candidates} á»©ng viÃªn phÃ¹ há»£p ({(suitable_candidates / total_candidates * 100):.1f}%)" if total_candidates > 0 else "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½"
        }
        
        # Save report
        report_file = self.data_path / f'bao_cao_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c táº¡o vÃ  lÆ°u táº¡i {report_file}")
        return report


def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y á»©ng dá»¥ng
    """
    parser = argparse.ArgumentParser(description='Há»‡ thá»‘ng Há»— trá»£ Ra quyáº¿t Ä‘á»‹nh Tuyá»ƒn dá»¥ng')
    parser.add_argument('--mode', choices=['train', 'predict', 'batch', 'demo'], 
                       default='demo', help='Cháº¿ Ä‘á»™ cháº¡y há»‡ thá»‘ng')
    parser.add_argument('--input', type=str, help='File Ä‘áº§u vÃ o cho dá»± Ä‘oÃ¡n hÃ ng loáº¡t')
    parser.add_argument('--model-path', type=str, default='models/', 
                       help='ÄÆ°á»ng dáº«n thÆ° má»¥c mÃ´ hÃ¬nh')
    parser.add_argument('--data-path', type=str, default='data/', 
                       help='ÄÆ°á»ng dáº«n thÆ° má»¥c dá»¯ liá»‡u')
    
    args = parser.parse_args()
    
    # Initialize system
    hr_system = HRDecisionSupportSystem(args.model_path, args.data_path)
    
    if args.mode == 'train':
        logger.info("ğŸ§  Cháº¿ Ä‘á»™ huáº¥n luyá»‡n Ä‘Æ°á»£c chá»n")
        accuracy = hr_system.train_model()
        print(f"ğŸ‰ Huáº¥n luyá»‡n mÃ´ hÃ¬nh hoÃ n táº¥t vá»›i Ä‘á»™ chÃ­nh xÃ¡c: {accuracy:.3f}")
    
    elif args.mode == 'predict':
        logger.info("ğŸ¯ Cháº¿ Ä‘á»™ dá»± Ä‘oÃ¡n Ä‘Æ¡n")
        # Example candidate
        candidate = {
            'candidate_id': 'DEMO_001',
            'years_experience': 5,
            'education_level': 'bachelor',
            'skills': 'python, machine learning, sql, data analysis, teamwork',
            'experience_description': 'ChuyÃªn gia phÃ¢n tÃ­ch dá»¯ liá»‡u cÃ³ kinh nghiá»‡m vá»›i ká»¹ nÄƒng láº­p trÃ¬nh máº¡nh',
            'position_applied': 'analyst'
        }
        
        result = hr_system.predict_candidate(candidate)
        print("ğŸ“‹ Káº¿t quáº£ dá»± Ä‘oÃ¡n:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    elif args.mode == 'batch':
        if not args.input:
            print("âŒ Lá»—i: Cáº§n file --input cho cháº¿ Ä‘á»™ dá»± Ä‘oÃ¡n hÃ ng loáº¡t")
            return
        
        logger.info(f"ğŸ‘¥ Cháº¿ Ä‘á»™ dá»± Ä‘oÃ¡n hÃ ng loáº¡t cho file: {args.input}")
        results = hr_system.batch_predict(args.input)
        report = hr_system.generate_report(results)
        print("âœ… Dá»± Ä‘oÃ¡n hÃ ng loáº¡t hoÃ n táº¥t")
        print("ğŸ“Š BÃ¡o cÃ¡o tá»•ng há»£p:")
        print(json.dumps(report, indent=2, ensure_ascii=False))
    
    else:  # demo mode
        logger.info("ğŸ® Cháº¿ Ä‘á»™ demo - Huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­")
        
        # Train model
        accuracy = hr_system.train_model()
        print(f"ğŸ‰ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i Ä‘á»™ chÃ­nh xÃ¡c: {accuracy:.3f}")
        
        